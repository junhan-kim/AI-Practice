{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "car_fire_detection_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF-D-tAxnGhQ",
        "colab_type": "text"
      },
      "source": [
        "# Car Fire Detection\n",
        "\n",
        "Using the pre-trained DenseNet and the added layer, learn images related to car fires.\n",
        "\n",
        "For learning, 1,000 images of accident and 1,000 images of non-accident were used, so total 2,000 images were used.\n",
        "\n",
        "See the comments for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzWQRu6OZ9mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import pathlib\n",
        "\n",
        "train_dir = pathlib.Path('./images/train')   # load train images\n",
        "validation_dir = pathlib.Path('./images/validation')  # load validation images\n",
        "train_image_count = len(list(train_dir.glob('*/*.jpg')))\n",
        "validation_image_count = len(list(validation_dir.glob('*/*.jpg')))\n",
        "print(train_image_count)\n",
        "print(validation_image_count)\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in train_dir.glob('*')])\n",
        "CLASS_NAMES\n",
        "\n",
        "accidents = list(train_dir.glob('accident/*'))\n",
        "\n",
        "for image_path in accidents[:3]:\n",
        "    display.display(Image.open(str(image_path)))\n",
        "\n",
        "# apply horizontal flip for data augmentation\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, horizontal_flip=True) \n",
        "validation_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, horizontal_flip=True) \n",
        "\n",
        "BATCH_SIZE = 32   # batch size is 32\n",
        "IMG_HEIGHT = 160\n",
        "IMG_WIDTH = 160\n",
        "STEPS_PER_EPOCH = np.ceil(train_image_count/BATCH_SIZE)  # epoch for each learning step \n",
        "VALIDATION_STEPS = np.ceil(validation_image_count/BATCH_SIZE)\n",
        "print(STEPS_PER_EPOCH)\n",
        "print(VALIDATION_STEPS)\n",
        "\n",
        "# loading images from directory \n",
        "train_data_gen = image_generator.flow_from_directory(directory='./images/train', \n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='binary')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(directory='./images/validation', \n",
        "                                                              batch_size=BATCH_SIZE,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')\n",
        "                        \n",
        "        \n",
        "def create_model():\n",
        "    IMG_SHAPE = (160,160,3)   # input images are 160 x 160 x 3 (W x H x C)\n",
        "\n",
        "    base_model = tf.keras.applications.DenseNet201(   # using pretrained model : DenseNet201\n",
        "        include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE,\n",
        "        pooling='avg', classes=1000\n",
        "    )\n",
        "\n",
        "    base_model.trainable = True    # base model should also be learned\n",
        "\n",
        "    model = tf.keras.Sequential([   # add some layers\n",
        "    base_model,\n",
        "    tf.keras.layers.Dense(1920, activation='relu'),  \n",
        "    tf.keras.layers.Dropout(0.2),  # in order for avoiding overfitting, apply dropout\n",
        "    tf.keras.layers.Dense(1)  # output layer must be 1 since result will be binary (0 : accident, 1 : non accident)\n",
        "    ])\n",
        "\n",
        "    base_learning_rate = 0.0001  # learning rate is 0.0001 (it's a empirically derived value)\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),   # use optimizer with Adam\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),   # loss function is binary cross entropy\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "len(model.trainable_variables)\n",
        "\n",
        "# training model \n",
        "model.fit(train_data_gen, batch_size=BATCH_SIZE, validation_data=val_data_gen, steps_per_epoch=STEPS_PER_EPOCH, validation_steps=VALIDATION_STEPS)\n",
        "\n",
        "# save model in a specific directory\n",
        "model.save('./model', save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}